{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bda93ba",
   "metadata": {
    "id": "9bda93ba"
   },
   "source": [
    "# <center>        **Introduction to Data Science**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d80c60",
   "metadata": {
    "id": "f5d80c60"
   },
   "source": [
    "# 1. Business Understanding\n",
    "\n",
    "Students are expected to identify an analytical problem of your choice. You have to detail the Business Understanding part of your problem under this heading which basically addresses the following questions.\n",
    "\n",
    "   1. What is the business problem that you are trying to solve?\n",
    "   2. What data do you need to answer the above problem?\n",
    "   3. What are the different sources of data?\n",
    "   4. What kind of analytics task are you performing?\n",
    "\n",
    "Score: 1 Mark in total (0.25 mark each)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3298e121",
   "metadata": {
    "id": "3298e121"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**1. What is the business problem that you are trying to solve?**\n",
    "\n",
    "> _As part of expanding the business of a commercial bank, they are planning to introduce different range of credit card offers to their customers. Now they wanted to market their offers and services to the targeted customers based on their financial capacity. The bank has collected various types of data of the potential customers that help them to predict their customers financial capacity. Predicted income then would be used to map the right credit card packages and the set the card features such as credit limit, reward schemes etc._\n",
    "\n",
    "**2. What data do you need to answer the above problem?**\n",
    "\n",
    "> _We need Individual's data which include Gender, Age, education details, employment status, income details, Dependents._\n",
    "\n",
    "**3. What are the different sources of data?**\n",
    "\n",
    "> _Bank has data set of existing customers and feature set that can be used for the training. For demonstration purpose we will be using the publicly available dataset._\n",
    "\n",
    "**4. What kind of analytics task are you performing?**\n",
    "\n",
    "> _We will be using Predictive analytics to suggest the income of individual based on the dataset provided._\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8e0cb",
   "metadata": {
    "id": "3cc8e0cb"
   },
   "source": [
    "# 2. Data Acquisition\n",
    "\n",
    "For the problem identified , find an appropriate data set (Your data set must\n",
    "be unique with minimum **20 features and 10k rows**) from any public data source.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 2.1 Download the data directly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b51d895",
   "metadata": {
    "id": "4b51d895",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "dataset = 'kamaumunyori/income-prediction-dataset-us-20th-century-data'\n",
    "dirpath = os.getcwd()\n",
    "\n",
    "# Setup kaggle authentication configuration\n",
    "help_msg = \"\"\"\n",
    "Setup kaggle Username and token\n",
    "\n",
    "    i.   login to https://www.kaggle.com/\n",
    "    ii.  go to profile (top right corner) -> settings\n",
    "    iii. click on \"Create New Token\"\n",
    "    iv.  kaggle.json will be downloaded with your username and key\n",
    "    v.   Copy to ~/.kaggle/ directory\n",
    "    vi.  chmod 600 ~/.kaggle/kaggle.json\n",
    "\"\"\"    \n",
    "\n",
    "# 2. Authentication    \n",
    "api = KaggleApi()\n",
    "try:\n",
    "    api.authenticate()\n",
    "except Exception as error: \n",
    "    print('Download_Dataset: Authentication failure', error)\n",
    "    print(help_msg)\n",
    "    raise\n",
    "\n",
    "# 3. Download all files from given dataset URL to dirpath if not present.\n",
    "try:\n",
    "    api.dataset_download_files(dataset=dataset, path=dirpath, unzip=True, quiet=False)\n",
    "    print('Download Completed !!!')\n",
    "except Exception as error:\n",
    "    print('Download_Datset: failed to download files:', error)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49530d0c",
   "metadata": {
    "id": "49530d0c"
   },
   "source": [
    "## 2.2 Code for converting the above downloaded data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4c171",
   "metadata": {
    "id": "c1f4c171",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pds\n",
    "import numpy as np\n",
    "import platform\n",
    "\n",
    "# Convert the dataset in CSV to dataframe\n",
    "if \"macos\" in platform.platform().lower():\n",
    "    # For MAC OS, the name and slash is different\n",
    "    Datasetfile = dirpath + \"/Income Prediction./Train.csv\"\n",
    "    Test_Datasetfile = dirpath + \"/Income Prediction./Test.csv\"\n",
    "else:\n",
    "    # For Windows OS\n",
    "    Datasetfile = dirpath + \"\\\\Income prediction\\\\Train.csv\"\n",
    "    Test_Datasetfile = dirpath + \"\\\\Income prediction\\\\Test.csv\"\n",
    "\n",
    "try:\n",
    "    Dataframe = pds.read_csv(Datasetfile, skipinitialspace=True)\n",
    "    Test_Dataframe = pds.read_csv(Test_Datasetfile, skipinitialspace=True)\n",
    "    print(\"Data Frame conversion: successful\")\n",
    "except Exception as error:\n",
    "    print('Data Frame conversion: failed:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1fea4d",
   "metadata": {
    "id": "7b1fea4d"
   },
   "source": [
    "## 2.3 Confirm the data has been correctly by displaying the first 5 and last 5 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e6c58",
   "metadata": {
    "id": "624e6c58",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display all columns without truncation\n",
    "pds.set_option('display.max_columns', None)\n",
    "\n",
    "# Few function like Replace, Fillna, etc in Pandas gives Future DeprecationWarning, fixing it to work like older pandas version.\n",
    "Dataframe = Dataframe.infer_objects(copy=False)\n",
    "Test_Dataframe = Test_Dataframe.infer_objects(copy=False)\n",
    "\n",
    "print(\"First 5 records in data set\")\n",
    "display(Dataframe.head(5))\n",
    "\n",
    "print(\"Last 5 records in data set\")\n",
    "display(Dataframe.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb84fc56",
   "metadata": {
    "id": "bb84fc56"
   },
   "source": [
    "## 2.4 Display the column headings, statistical information, description and statistical summary of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ad28e",
   "metadata": {
    "id": "086ad28e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Displaying column headings\n",
    "print(\"Displaying column headings:\")\n",
    "display(Dataframe.columns)\n",
    "\n",
    "# 5 point statistical summary of Dataset\n",
    "print(\"Statistical Information, description and statistical summary of the dataset:\")\n",
    "display(Dataframe.describe())\n",
    "Dataframe.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812edb18",
   "metadata": {
    "id": "812edb18"
   },
   "source": [
    "## 2.5 Write your observations from the above.\n",
    "1. Size of the dataset\n",
    "2. What type of data attributes are there?\n",
    "3. Is there any null data that has to be cleaned?\n",
    "\n",
    "Score: 2 Marks in total (0.25 marks for 2.1, 0.25 marks for 2.2, 0.5 marks for 2.3, 0.25 marks for 2.4, 0.75 marks for 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c49b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For 2.5 (3): Null values in each column which needs to be cleaned:\n",
    "print('Columns Containing Null Entries for Train Data')\n",
    "display(Dataframe.isnull().sum(axis=0))\n",
    "print('\\nColumns Containing Null Entries for Test Data')\n",
    "display(Test_Dataframe.isnull().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb64cf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grouped based on Attribute types\n",
    "attribute_types = {\n",
    "    'NOMINAL': ['ID', 'gender', 'class', 'marital_status', 'race', 'is_hispanic', 'is_labor_union', 'industry_code',\n",
    "                'industry_code_main', 'occupation_code', 'occupation_code_main', 'citizenship', 'country_of_birth_own',\n",
    "                'country_of_birth_father', 'country_of_birth_mother', 'migration_code_change_in_msa',\n",
    "                'migration_code_move_within_reg', 'migration_code_change_in_reg', 'old_residence_reg',\n",
    "                'old_residence_state','unemployment_reason'\n",
    "                ],\n",
    "    'ORDINAL': ['education', 'education_institute', 'employment_commitment', 'employment_stat', 'household_stat',\n",
    "                'household_summary', 'under_18_family', 'veterans_admin_questionnaire', 'vet_benefit', 'tax_status',\n",
    "                'migration_prev_sunbelt', 'residence_1_year_ago', 'income_above_limit'],\n",
    "    'INTERVAL': ['mig_year'],\n",
    "    'RATIO': ['age', 'wage_per_hour', 'working_week_per_year', 'total_employed', 'gains', 'losses', 'stocks_status',\n",
    "              'importance_of_record']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d80d2f",
   "metadata": {
    "id": "60d80d2f"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. Number of Rows = 209499, Number of attributes/columns = 43\n",
    "2. Type of Data Attributes out of 43:\n",
    "   <ul>\n",
    "   <li> Integer   :  12 </li>\n",
    "   <li> Float     :  1 </li>\n",
    "   <li> Object    :  30 </li>\n",
    "   </ul>\n",
    "   <b><i>Please refer the type of Data attributes identified above in {{ attribute_types }}</i></b>\n",
    "\n",
    "3. There are 14 Columns/attributes containing entries with NULL data which needs Data cleaning.\n",
    "   All such columns and number of null entry is captured in 2.4 Result.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e0e36",
   "metadata": {
    "id": "102e0e36"
   },
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cb469",
   "metadata": {
    "id": "637cb469"
   },
   "source": [
    "If input data is numerical or categorical, do 3.1, 3.2 and 3.4\n",
    "If input data is text, do 3.3 and 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcb953b",
   "metadata": {
    "id": "2bcb953b"
   },
   "source": [
    "## 3.1 Check for\n",
    "\n",
    "* duplicate data\n",
    "* missing data\n",
    "* data inconsistencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d2316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Printing the unique values in each column\n",
    "for colname, coltype in Dataframe.dtypes.items():\n",
    "    if coltype == object:\n",
    "        if Dataframe[colname].nunique() < 51:\n",
    "            print(\"Column name: {} Unique Values count: {}\\n\".format(colname, Dataframe[colname].nunique()))\n",
    "            print(\"Unique Values:{}\\n\".format(Dataframe[colname].unique()))\n",
    "        else:\n",
    "            print(\"Column name: {} Unique Values count: {}\\n\".format(colname, Dataframe[colname].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d2fe91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify Duplicate records\n",
    "try:\n",
    "    if Dataframe.duplicated().sum():\n",
    "        print(\"Train: Total Duplicate Rows Identified:\\n\", Dataframe.duplicated().sum())\n",
    "    else:\n",
    "        print(\"Train: No Duplicate Rows Identified\\n\")\n",
    "except Exception as error:\n",
    "    print('Train: DataFrame Failed to Identify Duplicate Rows', error)\n",
    "\n",
    "# Identify Duplicate records for Test Data\n",
    "try:\n",
    "    if Test_Dataframe.duplicated().sum():\n",
    "        print(\"Test: Total Duplicate Rows Identified:\\n\", Test_Dataframe.duplicated().sum())\n",
    "    else:\n",
    "        print(\"Test: No Duplicate Rows Identified\\n\")\n",
    "except Exception as error:\n",
    "    print('Test: DataFrame Failed to Identify Duplicate Rows', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3efa5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify the columns/attributes with Missing data\n",
    "try:\n",
    "    na_column_list = Dataframe.columns[Dataframe.isnull().any()].to_list()\n",
    "    na_column_dict = dict()\n",
    "    for column in na_column_list:\n",
    "        na_column_dict[column] = Dataframe[column].isnull().sum()\n",
    "    print(\"Train: Following columns Identified with Missing entries:\\n\", na_column_dict)\n",
    "except Exception as error:\n",
    "    print('Train: DataFrame Failed to Identify Missing Data', error)\n",
    "\n",
    "# Identify the columns/attributes with Missing data for Test Data\n",
    "try:\n",
    "    na_column_list = Test_Dataframe.columns[Test_Dataframe.isnull().any()].to_list()\n",
    "    na_column_dict = dict()\n",
    "    for column in na_column_list:\n",
    "        na_column_dict[column] = Test_Dataframe[column].isnull().sum()\n",
    "    print(\"Test: Following columns Identified with Missing entries:\\n\", na_column_dict)\n",
    "except Exception as error:\n",
    "    print('Test: DataFrame Failed to Identify Missing Data', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a2a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify Inconsistent data\n",
    "try:\n",
    "    import re\n",
    "    numeric_consistent_flag = True # Assuming numeric data is consistent\n",
    "    for colname, coltype in Dataframe.dtypes.items():\n",
    "        if colname in attribute_types['NOMINAL'] + attribute_types['ORDINAL']:\n",
    "            if coltype == object and colname != 'ID':\n",
    "                inspect = Dataframe[colname].unique()\n",
    "                display(inspect)\n",
    "                # 1. check for special characters.\n",
    "                pattern=re.compile('[@_!#$%^&*()<>?/\\\\|}{~:]')\n",
    "                df = Dataframe[colname].str.contains(pattern)\n",
    "                print(\"colname: {} contain special character :{}\\n\".format(colname,df.unique()))\n",
    "        # All the Numeric attributes must be in integer or float datatype\n",
    "        if colname in attribute_types['INTERVAL'] + attribute_types['RATIO']:\n",
    "            if not pds.api.types.is_numeric_dtype(coltype):\n",
    "                print(\"Inconsistent Column: \", colname, \" => number of entries:\", Dataframe[colname].str.isalnum().sum() - Dataframe[colname].str.isdigit().sum())\n",
    "                numeric_consistent_flag = False # Found numeric data is inconsistent\n",
    "except Exception as error:\n",
    "    print('DataFrame failed to identify Inconsistent data', error)\n",
    "\n",
    "if (numeric_consistent_flag):\n",
    "    print(\"Numeric data attribute is consistent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fdebf8",
   "metadata": {
    "id": "06fdebf8"
   },
   "source": [
    "## 3.2 Apply techniques\n",
    "* to remove duplicate data\n",
    "* to impute or remove missing data\n",
    "* to remove data inconsistencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11df60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove the ID to effective prediction\n",
    "Dataframe.drop(['ID'], axis=1, inplace=True)\n",
    "Test_Dataframe.drop(['ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3118eb",
   "metadata": {
    "id": "dd3118eb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Duplicate Rows Technique: Remove\n",
    "# Removing duplicates in Original dataset.\n",
    "try:\n",
    "    if Dataframe.duplicated().sum():\n",
    "        Dataframe.drop_duplicates(inplace=True)\n",
    "        print(\"Duplicates removed\")\n",
    "    else:\n",
    "        print(\"No duplicates found. No action needed\")\n",
    "except Exception as error:\n",
    "    print('DataFrame duplicate Removal Failed', error)\n",
    "\n",
    "# Test dataset\n",
    "try:\n",
    "    if Test_Dataframe.duplicated().sum():\n",
    "        Test_Dataframe.drop_duplicates(inplace=True)\n",
    "        print(\"TestDataframe: Duplicates removed\")\n",
    "    else:\n",
    "        print(\"TestDataframe: No duplicates found. No action needed\")\n",
    "except Exception as error:\n",
    "    print('TestDataframe: DataFrame duplicate Removal Failed', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433e393c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Missing Data Technique: Drop (Optional to use)\n",
    "try:\n",
    "    # Delete the columns with 66% missing entries\n",
    "    Dataframe.dropna(axis=1, thresh=((len(Dataframe)*2) / 3), inplace=True)\n",
    "    drop = False # Whether or not to drop the entire row if the attribute value is empty.\n",
    "    if drop:\n",
    "        # Drop missing Rows\n",
    "        Dataframe.dropna(axis=0, subset=attribute_types['INTERVAL'] + attribute_types['RATIO'], inplace=True)\n",
    "    print(\"Dropped missing data successfully\")\n",
    "    Dataframe.info(verbose=True)\n",
    "except Exception as error:\n",
    "    print(\"Missing data drop failure\", error)\n",
    "\n",
    "# Test Dataset\n",
    "try:\n",
    "    # Delete the columns with 66% missing entries\n",
    "    Test_Dataframe.dropna(axis=1, thresh=((len(Test_Dataframe)*2) / 3), inplace=True)\n",
    "    drop = False # Whether or not to drop the entire row if the attribute value is empty.\n",
    "    if drop:\n",
    "        # Drop missing Rows\n",
    "        Test_Dataframe.dropna(axis=0, subset=attribute_types['INTERVAL'] + attribute_types['RATIO'], inplace=True)\n",
    "    print(\"TestDataframe: Dropped missing data successfully\")\n",
    "    Test_Dataframe.info(verbose=True)\n",
    "except Exception as error:\n",
    "    print(\"TestDataframe: Missing data drop failure\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb9b88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inconsistent data Technique: Fix the format/Delete the inappropriate data.\n",
    "# Inppropriate data which is deleted will be filled with some technique in Missing value fill technique.\n",
    "\n",
    "def is_outlier_present(df, column):\n",
    "    df_temp = df.sort_values(by=[column])\n",
    "    Q1 = df_temp[column].quantile(0.25)\n",
    "    Q3 = df_temp[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    minimum = Q1 - 1.5 * IQR\n",
    "    maximum = Q3 + 1.5 * IQR\n",
    "    outliers = df_temp[(df_temp[column] < minimum) | (df_temp[column] > maximum)]\n",
    "    # If outliers is empty => No outliers\n",
    "    # If outliers is non-empty => Outliers present\n",
    "    return len(outliers) != 0\n",
    "\n",
    "unknown_pattern='Unknown'\n",
    "# Train Dataframe\n",
    "try:\n",
    "    for colname, coltype in Dataframe.dtypes.items():\n",
    "        # Categorical Attribute\n",
    "        if colname in attribute_types['NOMINAL'] + attribute_types['ORDINAL']:\n",
    "            # Remove Data Inconsistency from each column.\n",
    "            # Remove '?' and replace with 'Unknown'. This is a valid data\n",
    "            try:\n",
    "                Dataframe.replace(['?'], unknown_pattern, inplace=True)\n",
    "                print(\"column name {}: ? substituted with {} times\".format(colname, Dataframe[colname].value_counts()[unknown_pattern]))\n",
    "            except Exception as error:\n",
    "                print(\"column name {}: no match for ? {}\".format(colname, error))\n",
    "            #\n",
    "            # Drop the column if unknown entries are more than 45%.\n",
    "            # 45% considered because of some columns are just short of 50%.\n",
    "            #\n",
    "            try:\n",
    "                unknown_count = Dataframe[colname].value_counts()[unknown_pattern]\n",
    "                if (unknown_count > (len(Dataframe)*0.45)):\n",
    "                    print(\"{} dropped as half of the values are unknown\".format(colname))\n",
    "                    Dataframe.drop([colname], axis=1, inplace=True)\n",
    "            except Exception as error:\n",
    "                print(\"column name {}: no match for Unknown\".format(colname))\n",
    "        # Numerical Attribute\n",
    "        if colname in attribute_types['INTERVAL'] + attribute_types['RATIO']:\n",
    "            if coltype == object:\n",
    "                # This will remove all inconsistent data.\n",
    "                Dataframe[colname] = pds.to_numeric(pds.Series(Dataframe[colname]), errors='coerce', downcast=None)\n",
    "            if coltype != object:\n",
    "                # After converting to numeric, check for outliers\n",
    "                if is_outlier_present(Dataframe, colname):\n",
    "                    if colname in ['mig_year', 'age', 'working_week_per_year', 'total_employed']:\n",
    "                        # No Outliers found, handling is not done since there is no outliers found\n",
    "                        raise(\"Outlier exists in column: \", colname)\n",
    "                else:\n",
    "                    # Outliers drop technique cannot be used for below attributes:\n",
    "                    # Since by applying IQR technique, we are getting outliers for below attributes with larger frequencies.\n",
    "                    # Removing this would negatively affect the ML technique. Hence we are not applying IQR technique to drop below outliers.\n",
    "                    # wage_per_hour: 11856\n",
    "                    # gains: 7830\n",
    "                    # losses: 4062\n",
    "                    # stocks_status: 22032\n",
    "                    # importance_of_record: 6759\n",
    "                    pass\n",
    "    # Dataframe.info(verbose=True)\n",
    "    print(\"Train: Successfully handled inconsistency\")\n",
    "except Exception as error:\n",
    "    print(\"Train: Inconsistent fix failure\", error)\n",
    "\n",
    "# Test Dataframe\n",
    "try:\n",
    "    for colname, coltype in Test_Dataframe.dtypes.items():\n",
    "        # Categorical Attribute\n",
    "        if colname in attribute_types['NOMINAL'] + attribute_types['ORDINAL']:\n",
    "            # Remove Data Inconsistency from each column.\n",
    "            # Remove '?' and replace with 'Unknown'. This is a valid data\n",
    "            try:\n",
    "                Test_Dataframe.replace(['?'], unknown_pattern, inplace=True)\n",
    "                print(\"TestDataframe: column name {}: ? substituted with {} times\".format(colname, Test_Dataframe[colname].value_counts()[unknown_pattern]))\n",
    "            except Exception as error:\n",
    "                print(\"TestDataframe: column name {}: no match for ? {}\".format(colname, error))\n",
    "            #\n",
    "            # Drop the column if unknown entries are more than 45%.\n",
    "            # 45% considered because of some columns are just short of 50%.\n",
    "            #\n",
    "            try:\n",
    "                unknown_count = Test_Dataframe[colname].value_counts()[unknown_pattern]\n",
    "                if (unknown_count > (len(Test_Dataframe)*0.45)):\n",
    "                    print(\"TestDataframe: {} dropped as half of the values are unknown\".format(colname))\n",
    "                    Test_Dataframe.drop([colname], axis=1, inplace=True)\n",
    "            except Exception as error:\n",
    "                print(\"TestDataframe: column name {}: no match for Unknown\".format(colname))\n",
    "        # Numerical Attribute\n",
    "        if colname in attribute_types['INTERVAL'] + attribute_types['RATIO']:\n",
    "            if coltype == object:\n",
    "                # This will remove all inconsistent data.\n",
    "                Test_Dataframe[colname] = pds.to_numeric(pds.Series(Test_Dataframe[colname]), errors='coerce', downcast=None)\n",
    "            if coltype != object:\n",
    "                # After converting to numeric, check for outliers\n",
    "                if is_outlier_present(Test_Dataframe, colname):\n",
    "                    if colname in ['mig_year', 'age', 'working_week_per_year', 'total_employed']:\n",
    "                        # No Outliers found, handling is not done since there is no outliers found\n",
    "                        raise(\"TestDataframe: Outlier exists in column: \", colname)\n",
    "                else:\n",
    "                    # Outliers drop technique cannot be used for below attributes:\n",
    "                    # Since by applying IQR technique, we are getting outliers for below attributes with larger frequencies.\n",
    "                    # Removing this would negatively affect the ML technique. Hence we are not applying IQR technique to drop below outliers.\n",
    "                    # wage_per_hour: 11856\n",
    "                    # gains: 7830\n",
    "                    # losses: 4062\n",
    "                    # stocks_status: 22032\n",
    "                    # importance_of_record: 6759\n",
    "                    pass\n",
    "    # Test_Dataframe.info(verbose=True)\n",
    "    print(\"TestDataframe: Successfully handled inconsistency\")\n",
    "except Exception as error:\n",
    "    print(\"TestDataframe: Inconsistent fix failure\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c005a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Missing Data Technique: Impute with measure of central tendency\n",
    "try:\n",
    "    Dataframe.info(verbose=True)\n",
    "    for colname, coltype in Dataframe.dtypes.items():\n",
    "        # Categorical Attribute\n",
    "        if colname in attribute_types['NOMINAL'] + attribute_types['ORDINAL']:\n",
    "            # Impute missing data with mode\n",
    "            if coltype == object and colname != 'ID':\n",
    "                try:\n",
    "                    # Dataframe[colname].fillna(Dataframe[colname].mode(dropna=True)[0], inplace=True)\n",
    "                    Dataframe.fillna({colname: Dataframe[colname].mode(dropna=True)[0]}, inplace=True)\n",
    "                except Exception as error:\n",
    "                    print(\"Categorical mode imputing failed for {}, error {} \".format(colname, error))\n",
    "                #debug log to verify we dont have unknown\n",
    "                print(\"column name: {} mode: {}\\n\".format(colname, Dataframe[colname].mode(dropna=True)[0]))\n",
    "        # Numerical Attribute\n",
    "        if colname in attribute_types['INTERVAL'] + attribute_types['RATIO']:\n",
    "            # Impute missing data with mean\n",
    "            Dataframe[colname] = Dataframe[colname].fillna(Dataframe[colname].mean())\n",
    "    print('Imputed missing and inconsistent data successfully')\n",
    "except Exception as error:\n",
    "    print(\"Missing data impute failure\", error)\n",
    "\n",
    "# Test Dataframe\n",
    "try:\n",
    "    Test_Dataframe.info(verbose=True)\n",
    "    for colname, coltype in Test_Dataframe.dtypes.items():\n",
    "        # Categorical Attribute\n",
    "        if colname in attribute_types['NOMINAL'] + attribute_types['ORDINAL']:\n",
    "            # Impute missing data with mode\n",
    "            if coltype == object and colname != 'ID':\n",
    "                try:\n",
    "                    # Test_Dataframe[colname].fillna(Test_Dataframe[colname].mode(dropna=True)[0], inplace=True)\n",
    "                    Test_Dataframe.fillna({colname: Test_Dataframe[colname].mode(dropna=True)[0]}, inplace=True)\n",
    "                except Exception as error:\n",
    "                    print(\"TestDataframe: Categorical mode imputing failed for {}, error {} \".format(colname, error))\n",
    "                #debug log to verify we dont have unknown\n",
    "                print(\"TestDataframe: column name: {} mode: {}\\n\".format(colname, Test_Dataframe[colname].mode(dropna=True)[0]))\n",
    "        # Numerical Attribute\n",
    "        if colname in attribute_types['INTERVAL'] + attribute_types['RATIO']:\n",
    "            # Impute missing data with mean\n",
    "            Test_Dataframe[colname] = Test_Dataframe[colname].fillna(Test_Dataframe[colname].mean())\n",
    "    print('TestDataframe: Imputed missing and inconsistent data successfully')\n",
    "except Exception as error:\n",
    "    print(\"TestDataframe: Missing data impute failure\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46394034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Duplicate Rows Technique: Remove\n",
    "# Removing duplicates once the missing and inconsistent data is filled.\n",
    "try:\n",
    "    if Dataframe.duplicated().sum():\n",
    "        Dataframe.drop_duplicates(inplace=True)\n",
    "        print(\"Duplicates removed\")\n",
    "    else:\n",
    "        print(\"No Duplicate found. No action needed\")\n",
    "except Exception as error:\n",
    "    print('DataFrame duplicate Removal Failed', error)\n",
    "\n",
    "# Removing duplicates once the missing and inconsistent data is filled in Test Dataframe.\n",
    "try:\n",
    "    if Test_Dataframe.duplicated().sum():\n",
    "        Test_Dataframe.drop_duplicates(inplace=True)\n",
    "        print(\"TestDataframe: Duplicates removed\")\n",
    "    else:\n",
    "        print(\"TestDataframe: No Duplicate found. No action needed\")\n",
    "except Exception as error:\n",
    "    print('TestDataframe: DataFrame duplicate Removal Failed', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139fedf",
   "metadata": {
    "id": "2139fedf"
   },
   "source": [
    "## 3.3 Encode categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6886fc",
   "metadata": {
    "id": "da6886fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregation and Label encoding\n",
    "edu_mapping = {'Children':0,'Less than 1st grade':0,'1st 2nd 3rd or 4th grade':0,'5th or 6th grade':0,'7th and 8th grade':0,\n",
    "                                         '9th grade':0,'10th grade':0, '11th grade':0,'12th grade no diploma':0,'High school graduate':1,'Some college but no degree':2,\n",
    "                                         'Associates degree-academic program':2,'Associates degree-occup /vocational':2,'Bachelors degree(BA AB BS)':2,'Prof school degree (MD DDS DVM LLB JD)':3,\n",
    "                                         'Masters degree(MA MS MEng MEd MSW MBA)':3,'Doctorate degree(PhD EdD)':3}\n",
    "tax = {'Head of household':2, 'Single':1, 'Nonfiler':0, 'Joint both 65+':5,\n",
    "       'Joint both under 65':3, 'Joint one under 65 & one 65+':4}\n",
    "hh_sum = {'Householder':2, 'Child 18 or older':1, 'Child under 18 never married':0,\n",
    "       'Spouse of householder':3, 'Nonrelative of householder':6,\n",
    "       'Other relative of householder':4,\n",
    "       'Group Quarters- Secondary individual':5,\n",
    "       'Child under 18 ever married':0}\n",
    "income_map = {'Below limit':0, 'Above limit':1}\n",
    "hh_stat = {'Householder':1, 'Nonfamily householder':0,\n",
    "       'Child 18+ never marr Not in a subfamily':5,\n",
    "       'Child <18 never marr not in subfamily':4, 'Spouse of householder':2,\n",
    "       'Child 18+ spouse of subfamily RP':5, 'Secondary individual':3,\n",
    "       'Child 18+ never marr RP of subfamily':5,\n",
    "       'Other Rel 18+ spouse of subfamily RP':5,\n",
    "       'Grandchild <18 never marr not in subfamily':4,\n",
    "       'Other Rel <18 never marr child of subfamily RP':4,\n",
    "       'Other Rel 18+ ever marr RP of subfamily':5,\n",
    "       'Other Rel 18+ ever marr not in subfamily':5,\n",
    "       'Child 18+ ever marr Not in a subfamily':5,\n",
    "       'RP of unrelated subfamily':0, 'Child 18+ ever marr RP of subfamily':5,\n",
    "       'Other Rel 18+ never marr not in subfamily':5,\n",
    "       'Child under 18 of RP of unrel subfamily':4,\n",
    "       'Grandchild <18 never marr child of subfamily RP':4,\n",
    "       'Grandchild 18+ never marr not in subfamily':5,\n",
    "       'Other Rel <18 never marr not in subfamily':4, 'In group quarters':6,\n",
    "       'Grandchild 18+ ever marr not in subfamily':5,\n",
    "       'Other Rel 18+ never marr RP of subfamily':5,\n",
    "       'Child <18 never marr RP of subfamily':4,\n",
    "       'Grandchild 18+ never marr RP of subfamily':5,\n",
    "       'Spouse of RP of unrelated subfamily':0,\n",
    "       'Grandchild 18+ ever marr RP of subfamily':5,\n",
    "       'Child <18 ever marr not in subfamily':4,\n",
    "       'Child <18 ever marr RP of subfamily':4,\n",
    "       'Other Rel <18 ever marr RP of subfamily':4,\n",
    "       'Grandchild 18+ spouse of subfamily RP':5,\n",
    "       'Child <18 spouse of subfamily RP':4,\n",
    "       'Other Rel <18 ever marr not in subfamily':4,\n",
    "       'Other Rel <18 never married RP of subfamily':4,\n",
    "       'Other Rel <18 spouse of subfamily RP':4,\n",
    "       'Grandchild <18 ever marr not in subfamily':4,\n",
    "       'Grandchild <18 never marr RP of subfamily':4}\n",
    "em_com = {'Not in labor force':0, 'Children or Armed Forces':6,\n",
    "       'Full-time schedules':5, 'PT for econ reasons usually PT':3,\n",
    "       'Unemployed full-time':1, 'PT for non-econ reasons usually FT':4,\n",
    "       'PT for econ reasons usually FT':4, 'Unemployed part- time':2}\n",
    "\n",
    "Dataframe = Dataframe.infer_objects(copy=False)\n",
    "Test_Dataframe = Test_Dataframe.infer_objects(copy=False)\n",
    "\n",
    "Dataframe['education']=Dataframe['education'].replace(edu_mapping)\n",
    "Dataframe['tax_status']=Dataframe['tax_status'].replace(tax)\n",
    "Dataframe['household_summary']=Dataframe['household_summary'].replace(hh_sum)\n",
    "Dataframe['income_above_limit']=Dataframe['income_above_limit'].replace(income_map)\n",
    "Dataframe['household_stat']=Dataframe['household_stat'].replace(hh_stat)\n",
    "Dataframe['employment_commitment']=Dataframe['employment_commitment'].replace(em_com)\n",
    "\n",
    "Test_Dataframe['education']=Test_Dataframe['education'].replace(edu_mapping)\n",
    "Test_Dataframe['tax_status']=Test_Dataframe['tax_status'].replace(tax)\n",
    "Test_Dataframe['household_summary']=Test_Dataframe['household_summary'].replace(hh_sum)\n",
    "Test_Dataframe['household_stat']=Test_Dataframe['household_stat'].replace(hh_stat)\n",
    "Test_Dataframe['employment_commitment']=Test_Dataframe['employment_commitment'].replace(em_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445b177-2322-482b-8462-9c0fdd2935fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregation and Label encoding continued\n",
    "#\n",
    "# gender mapping.\n",
    "#\n",
    "Dataframe['gender'] = Dataframe['gender'].map({\"Male\": 0, \"Female\": 1})\n",
    "Test_Dataframe['gender'] = Test_Dataframe['gender'].map({\"Male\": 0, \"Female\": 1})\n",
    "\n",
    "\n",
    "#\n",
    "# married status mapping rules.\n",
    "#\n",
    "marriage_mapping = {\"Married-civilian spouse present\": \"Married\",\n",
    "                    \"Married-spouse absent\": \"Married\",\n",
    "                    \"Married-A F spouse present\": \"Married\",\n",
    "                    \"Widowed\": \"Single\", \"Divorced\": \"Single\",\n",
    "                    \"Separated\": \"Single\", \"Never married\": \"Single\",\n",
    "}\n",
    "\n",
    "## Apply the mapping.\n",
    "Dataframe['marital_status'] = Dataframe['marital_status'].map(marriage_mapping)\n",
    "Test_Dataframe['marital_status'] = Test_Dataframe['marital_status'].map(marriage_mapping)\n",
    "\n",
    "## Further map to binary values.\n",
    "Dataframe[\"marital_status\"] = Dataframe[\"marital_status\"].map({\"Married\": 0, \"Single\": 1})\n",
    "Test_Dataframe[\"marital_status\"] = Test_Dataframe[\"marital_status\"].map({\"Married\": 0, \"Single\": 1})\n",
    "\n",
    "\n",
    "#\n",
    "# 'citizenship' mapping rules\n",
    "#\n",
    "citizenship_mapping = {\"Native\": \"Citizen\",\n",
    "                    \"Native- Born abroad of American Parent(s)\": \"Citizen\",\n",
    "                    \"Native- Born in Puerto Rico or U S Outlying\": \"Citizen\",\n",
    "                    \"Foreign born- U S citizen by naturalization\": \"Citizen\",\n",
    "                    \"Foreign born- Not a citizen of U S \": \"Non-citizen\"\n",
    "}\n",
    "## Apply the mapping.\n",
    "Dataframe['citizenship'] = Dataframe['citizenship'].map(citizenship_mapping)\n",
    "Test_Dataframe['citizenship'] = Test_Dataframe['citizenship'].map(citizenship_mapping)\n",
    "## Further map to binary values.\n",
    "Dataframe[\"citizenship\"] = Dataframe[\"citizenship\"].map({\"Citizen\": 0, \"Non-citizen\": 1})\n",
    "Test_Dataframe[\"citizenship\"] = Test_Dataframe[\"citizenship\"].map({\"Citizen\": 0, \"Non-citizen\": 1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5a2917",
   "metadata": {
    "id": "ae5a2917"
   },
   "source": [
    "## 3.4 Text data\n",
    "\n",
    "1. Remove special characters\n",
    "2. Change the case (up-casing and down-casing).\n",
    "3. Tokenization â€” process of discretizing words within a document.\n",
    "4. Filter Stop Words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b2cdee",
   "metadata": {
    "id": "a3b2cdee"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "There is no text data in our dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cec4fc",
   "metadata": {
    "id": "e3cec4fc"
   },
   "source": [
    "## 3.4 Report\n",
    "\n",
    "Mention and justify the method adopted\n",
    "* to remove duplicate data, if present\n",
    "* to impute or remove missing data, if present\n",
    "* to remove data inconsistencies, if present\n",
    "\n",
    "OR for textdata\n",
    "* How many tokens after step 3?\n",
    "* how may tokens after stop words filtering?\n",
    "\n",
    "If the any of the above are not present, then also add in the report below.\n",
    "\n",
    "Score: 2 Marks (based on the dataset you have, the data prepreation you had to do and report typed, marks will be distributed between 3.1, 3.2, 3.3 and 3.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab84ce6",
   "metadata": {
    "id": "3ab84ce6"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Deletion technique:**\n",
    "<li> Our dataset has unique ID's for all the records. Hence we didn't dropped the 'ID' column. </li>\n",
    "<li> We delete the columns with 66% missing entries. </li>\n",
    "\n",
    "**Inconsistent data:**\n",
    "<li> We removed the inconsistencies like \"?\" => \"Unknown\". </li>\n",
    "<li> If the Unknown is more than 45% in the column, then we drop the whole column. </li>\n",
    "<li> Check for outliers for appropriate attributes. </li>\n",
    "\n",
    "**Missing data:**\n",
    "<li> We impute the Method of Central tendency - Mode technique for Categorical Attributes other than Unknown value. </li>\n",
    "<li> We impute the Method of Central tendency - Mean technique for Numerical Attributes if any. </li>\n",
    "\n",
    "**Deletion technique:**\n",
    "<li> After all the above steps, we drop the duplicate records if any.</li>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fea7e8",
   "metadata": {
    "id": "f0fea7e8"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "There is no text data in our dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cd04b",
   "metadata": {
    "id": "793cd04b"
   },
   "source": [
    "## 3.5 Identify the target variables.\n",
    "\n",
    "* Separate the data from the target such that the dataset is in the form of (X,y) or (Features, Label)\n",
    "\n",
    "* Discretize / Encode the target variable or perform one-hot encoding on the target or any other as and if required.\n",
    "\n",
    "* Report the observations\n",
    "\n",
    "Score: 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54416b2c-bd6d-4be2-a900-fbcfc591b7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#discretize numerical features\n",
    "\n",
    "#\n",
    "# age\n",
    "#\n",
    "\n",
    "## Define bin edges\n",
    "bin_edges = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "## Create labels for the bins\n",
    "bin_labels = [\"0-10\", \"11-20\", \"21-30\", \"31-40\", \"41-50\",\n",
    "            \"51-60\", \"61-70\", \"71-80\", \"81-90\", \"91-100\"]\n",
    "\n",
    "## Apply the binning using pd.cut()\n",
    "Dataframe[\"age\"] = pds.cut(Dataframe[\"age\"], bins=bin_edges, labels=bin_labels)\n",
    "print(\"Train: {}\\n\".format(Dataframe['age'].value_counts()))\n",
    "Test_Dataframe[\"age\"] = pds.cut(Test_Dataframe[\"age\"], bins=bin_edges, labels=bin_labels)\n",
    "print(\"Test: {}\\n\".format(Test_Dataframe['age'].value_counts()))\n",
    "\n",
    "#\n",
    "# Hourly wage encoding.\n",
    "#\n",
    "\n",
    "## Define bin edges\n",
    "bin_edges = [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "\n",
    "## Create labels for the bins\n",
    "bin_labels = [\"0-1000\", \"1001-2000\", \"2001-3000\", \"3001-4000\", \"4001-5000\",\n",
    "            \"5001-6000\", \"6001-7000\", \"7001-8000\", \"8001-9000\", \"9001-9999\"]\n",
    "\n",
    "## Apply the binning using pd.cut()\n",
    "Dataframe[\"wage_per_hour\"] = pds.cut(Dataframe[\"wage_per_hour\"], bins=bin_edges, labels=bin_labels)\n",
    "print(\"Train: {}\\n\".format(Dataframe['wage_per_hour'].value_counts()))\n",
    "Test_Dataframe[\"wage_per_hour\"] = pds.cut(Test_Dataframe[\"wage_per_hour\"], bins=bin_edges, labels=bin_labels)\n",
    "print(\"Test: {}\\n\".format(Test_Dataframe['wage_per_hour'].value_counts()))\n",
    "\n",
    "#\n",
    "# Investment/capital market gains encoding.\n",
    "#\n",
    "\n",
    "# Define bin edges\n",
    "bin_edges = [0, 10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000]\n",
    "\n",
    "## Create labels for the bins\n",
    "bin_labels = [\"0-10000\", \"10001-20000\", \"20001-30000\", \"30001-40000\", \"40001-50000\",\n",
    "            \"50001-60000\", \"60001-70000\", \"70001-80000\", \"80001-90000\", \"90001-99999\"]\n",
    "\n",
    "## Apply the binning using pd.cut()\n",
    "Dataframe[\"gains\"] = pds.cut(Dataframe[\"gains\"], bins=bin_edges, labels=bin_labels)\n",
    "print(\"Train: {}\\n\".format(Dataframe['gains'].value_counts()))\n",
    "Test_Dataframe[\"gains\"] = pds.cut(Test_Dataframe[\"gains\"], bins=bin_edges, labels=bin_labels)\n",
    "print(\"Test: {}\\n\".format(Test_Dataframe['gains'].value_counts()))\n",
    "\n",
    "#\n",
    "# Investment/capital market stocks encoding.\n",
    "#\n",
    "\n",
    "# Define bin edges\n",
    "bin_edges = [0, 10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000]\n",
    "\n",
    "## Create labels for the bins\n",
    "bin_labels = [\"0-10000\", \"10001-20000\", \"20001-30000\", \"30001-40000\", \"40001-50000\",\n",
    "            \"50001-60000\", \"60001-70000\", \"70001-80000\", \"80001-90000\", \"90001-99999\"]\n",
    "\n",
    "## Apply the binning using pd.cut()\n",
    "Dataframe[\"stocks_status\"] = pds.cut(Dataframe[\"stocks_status\"], bins=bin_edges, labels=bin_labels)\n",
    "print(\"Train: {}\\n\".format(Dataframe['stocks_status'].value_counts()))\n",
    "Test_Dataframe[\"stocks_status\"] = pds.cut(Test_Dataframe[\"stocks_status\"], bins=bin_edges, labels=bin_labels)\n",
    "print(\"Test: {}\\n\".format(Test_Dataframe['stocks_status'].value_counts()))\n",
    "\n",
    "#\n",
    "# Investment/capital market gains encoding.\n",
    "#\n",
    "\n",
    "# Define bin edges\n",
    "bin_edges = [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "\n",
    "## Create labels for the bins\n",
    "bin_labels = [\"0-1000\", \"1001-2000\", \"2001-3000\", \"3001-4000\", \"4001-5000\",\n",
    "            \"5001-6000\", \"6001-7000\", \"7001-8000\", \"8001-9000\", \"9001-9999\"]\n",
    "\n",
    "## Apply the binning using pd.cut()\n",
    "Dataframe[\"losses\"] = pds.cut(Dataframe[\"losses\"], bins=bin_edges, labels=bin_labels)\n",
    "print(\"Train: {}\\n\".format(Dataframe['losses'].value_counts()))\n",
    "Test_Dataframe[\"losses\"] = pds.cut(Test_Dataframe[\"losses\"], bins=bin_edges, labels=bin_labels)\n",
    "print(\"Test: {}\\n\".format(Test_Dataframe['losses'].value_counts()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe6750-6c1a-4354-b914-a42c0ec43b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Final Coding Procedure.\n",
    "\n",
    "## Create a list of all the unique values in the feature\n",
    "categorical_columns = ['age', 'race', 'education', 'is_hispanic', \n",
    "                       'employment_commitment', 'working_week_per_year',\n",
    "                       'industry_code_main', 'household_stat', 'stocks_status',\n",
    "                       'household_summary', 'tax_status', 'citizenship', 'gains', 'losses',\n",
    "                       'country_of_birth_own', 'country_of_birth_father', 'country_of_birth_mother',\n",
    "                       'wage_per_hour',\n",
    "                      ]\n",
    "\n",
    "## Define function that codes the values into categorical codes\n",
    "\n",
    "def consistent_coding(dataframes, categorical_columns):\n",
    "    \"\"\"Applies consistent category coding across multiple DataFrames.\"\"\"\n",
    "\n",
    "    # Create a dictionary to store global category mappings\n",
    "    category_codes = {}\n",
    "\n",
    "    for df in dataframes:\n",
    "        for col in categorical_columns:\n",
    "            print(\"{} {} -> \".format(col, df[col].unique()))\n",
    "            if col not in category_codes:\n",
    "                # Create mapping for new categories\n",
    "                category_codes[col] = {\n",
    "                    value: i for i, value in enumerate(df[col].dropna().unique())\n",
    "                }\n",
    "\n",
    "            # Apply mapping to DataFrame\n",
    "            df[col] = df[col].astype('category')\n",
    "            df[col] = df[col].cat.set_categories(list(category_codes[col].keys()))\n",
    "            df[col] = df[col].cat.codes  # Assign codes based on global mapping\n",
    "            print(\" {}\\n\".format(df[col].unique()))\n",
    "    \n",
    "    df_category_codes = pds.DataFrame(category_codes)\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "## Apply consistent coding to training & testing data.\n",
    "#train_df_coded = consistent_coding([train_df, test_df], categorical_columns)\n",
    "train_df_coded = consistent_coding([Dataframe, Test_Dataframe], categorical_columns)\n",
    "# Train Dataframe\n",
    "display(train_df_coded[0].head(5))\n",
    "# Test Dataframe\n",
    "display(train_df_coded[1].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df71de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data target in (X, y)\n",
    "x = Dataframe.columns.values.tolist()\n",
    "y = ['income_above_limit']\n",
    "x = list(set(x).difference(y))\n",
    "print(\"(X_Features: {}, y_target: {}, target_label:{})\\n\".format(x,y,Dataframe[y[0]].unique()))\n",
    "\n",
    "print(\"(X_Features: {}, y_target: {}, target_label:{})\\n\".format(x,y,train_df_coded[0][y[0]].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f422cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Report:</b>\n",
    "\n",
    "1. Attribute: 'ID' removed from further Feature processing.\n",
    "2. y: Target variable 'income_above_limit' is categorical attribute and its Ordinal value is already encoded in section 3.3\n",
    "3. X: Out of 43 initial features we have filtered 28 attributes after data cleanup going for Data Exploration stage.   \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae0b5d2",
   "metadata": {
    "id": "3ae0b5d2"
   },
   "source": [
    "# 4. Data Exploration using various plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186bf4d7",
   "metadata": {
    "id": "186bf4d7"
   },
   "source": [
    "## 4.1 Scatter plot of each quantitative attribute with the target.\n",
    "\n",
    "Score: 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d7b27",
   "metadata": {
    "id": "868d7b27"
   },
   "outputs": [],
   "source": [
    "train_df = train_df_coded[0]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ignore_plot = ['ID','importance_of_record','mig_year', 'country_of_birth_father', 'occupation_code',\n",
    "               'country_of_birth_own','country_of_birth_mother','industry_code','industry_code_main',\n",
    "               'is_hispanic','losses', 'household_summary', 'wage_per_hour', 'employment_stat','house_hold_stat',\n",
    "               'race']\n",
    "try:\n",
    "    # random sampling from large dataset.\n",
    "    df = train_df.sample(1000)\n",
    "    for entry in x:\n",
    "        if entry not in ignore_plot:\n",
    "            fig = plt.figure(figsize=(25,4))\n",
    "            sns.scatterplot(data=df, x=entry, y ='income_above_limit',hue = 'income_above_limit')\n",
    "            plt.pause(1)\n",
    "            plt.close()\n",
    "            fig = plt.figure(figsize=(25,8))\n",
    "            sns.countplot(data=df, x=entry,hue = 'income_above_limit')\n",
    "            plt.pause(1)\n",
    "            plt.close() \n",
    "except Exception as error:\n",
    "    print('Failed to plot graph',error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63051251",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Observations from above plots:</b>\n",
    "\n",
    "1. 30 < Age < 65 are having more probability of having income_above_liimit as compared to other age group.\n",
    "2. Record having stock_status with more Gains and less losses is having high probability of income_above_limit as compared to without stocks.\n",
    "3. education: degree,masters,doctorate is having high probability of income_above_limit then others.\n",
    "4. total_employed > 6 is highest probability of income_above_limit then others.\n",
    "5. marital_status: married_civilian with spouse present is having highest probability of income_above_limit then others.\n",
    "6. citizenship/race : native/white has highest proability of income above limit the others.\n",
    "7. employment_commitment: fulltime and armed forces entry are having more probability of income_above_limit then others.\n",
    "8. vet_benefit : > 1 have more probability of income above limit then 0.\n",
    "9. gender: Male compare to Female are more in income_above_limit.\n",
    "10. working_week_per_year > 50 have more probability of income_above_limit then < 50.\n",
    "11. tax_status: Husband/wife < 65 working and paying tax has more probability of income above limit then others.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f9e37",
   "metadata": {
    "id": "575f9e37"
   },
   "source": [
    "## 4.2 EDA using visuals\n",
    "* Use (minimum) 2 plots (pair plot, heat map, correlation plot, regression plot...) to identify the optimal set of attributes that can be used for classification.\n",
    "* Name them, explain why you think they can be helpful in the task and perform the plot as well. Unless proper justification for the choice of plots given, no credit will be awarded.\n",
    "\n",
    "Score: 2 Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d614311",
   "metadata": {
    "id": "4d614311"
   },
   "outputs": [],
   "source": [
    "#Heat Map: To describe correlation (strength and direction) among several numerical/ordinal variables/attributes.\n",
    "corr_matrix=Dataframe.corr()\n",
    "#Creating a seaborn heatmap\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(corr_matrix,cmap='BrBG', center=0, annot=True, linewidths=0.5, linecolor='red')\n",
    "plt.pause(1)\n",
    "plt.close()\n",
    "#Heat Map: Interpretation:\n",
    "# working_week_per_year,total_employed,occupation_code,industry_code are having positive correlation.\n",
    "# tax_status,vet_benefit is having age,education,working_week_per_year,total_employed and household_summary postive correlation. \n",
    "# migration_prev_sunbelt and residence_1_year_ago is having postive corelation. \n",
    "# income_above_level is positively correlated with attributes: age,education,working_week_per_year,industry_code,total_employed,vet_benefit,tax_status,gains,losses,stock_status\n",
    "# house_hold_stat is negatively correlated with most of the attributes.\n",
    "# employment_stat is postively correlated with total_employed,working_week_per_year,industry_code,occupation_code.\n",
    "print(\"########################################################################################################\\n\")\n",
    "\n",
    "\n",
    "x_list = ['age', 'wage_per_hour', 'working_week_per_year', 'gains','losses', 'stocks_status', 'importance_of_record']\n",
    "y_list = [i for i in x if i not in x_list]\n",
    "# pair plot : To describe distribution of data and relationship between attributes. \n",
    "# Here exploring Nominal attribute relation with ratio/ordinal attributes.\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.pairplot(hue='income_above_limit', data=df, y_vars = y_list, x_vars = x_list)\n",
    "plt.pause(1)\n",
    "plt.close()\n",
    "\n",
    "#Pair plot Map: Interpretation:\n",
    "# age and marital status.\n",
    "# education and job class.\n",
    "# race/citizenship and with wage_per_hour, working_week_per_year.\n",
    "# employment commitment and wage_per_hour, working_week_per_year,total_employed.\n",
    "# All above identified attributes have relation and influence to predict target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc82a1",
   "metadata": {
    "id": "bdbc82a1"
   },
   "source": [
    "# 5. Data Wrangling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca214eb3",
   "metadata": {
    "id": "ca214eb3"
   },
   "source": [
    "## 5.1 Univariate Filters\n",
    "\n",
    "#### Numerical and Categorical Data\n",
    "* Identify top 5 significant features by evaluating each feature independently with respect to the target variable by exploring\n",
    "1. Mutual Information (Information Gain)\n",
    "2. Gini index\n",
    "3. Gain Ratio\n",
    "4. Chi-Squared test\n",
    "5. Fisher Score\n",
    "(From the above 5 you are required to use only any <b>two</b>)\n",
    "\n",
    "#### For Text data\n",
    "\n",
    "1. Stemming / Lemmatization.\n",
    "2. Forming n-grams and storing them in the document vector.\n",
    "3. TF-IDF\n",
    "(From the above 2 you are required to use only any <b>two</b>)\n",
    "\n",
    "\n",
    "Score: 3 Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f07bbd6-a7ac-417a-8868-50a8b93ce15a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IG calculation\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "train_df1 = train_df_coded[0].copy()\n",
    "\n",
    "display(train_df1.head())\n",
    "train_df1.info(verbose=True)\n",
    "for colname, coltype in train_df1.dtypes.items():\n",
    "    print(\"Column name: {} Unique Values count: {}\\n\".format(colname, train_df1[colname].nunique()))\n",
    "    \n",
    "x = ['age', 'gender', 'education', 'marital_status', 'race', 'is_hispanic', 'employment_commitment', 'employment_stat', \n",
    "     'wage_per_hour', 'working_week_per_year', 'industry_code', 'industry_code_main', 'occupation_code', 'total_employed', \n",
    "     'household_stat', 'household_summary', 'vet_benefit', 'tax_status', 'gains', 'losses', 'stocks_status', 'citizenship', \n",
    "     'mig_year', 'country_of_birth_own', 'country_of_birth_father', 'country_of_birth_mother', 'importance_of_record']\n",
    "\n",
    "x_discrete = [True, True, True, True, True, True, True, True, \n",
    "             True, False, False, False, False, True, \n",
    "             True, True, True, True, True, True, True, True, \n",
    "             False, False, False, False, False]\n",
    "\n",
    "Y = train_df_coded[0]['income_above_limit']\n",
    "#remove target\n",
    "train_df1.pop('income_above_limit')\n",
    "print(\"Set X and Y. Computing Information Gain...\\n\")\n",
    "\n",
    "\n",
    "mi = mutual_info_classif(train_df1, Y, discrete_features=x_discrete)\n",
    "print(\"mi {}\".format(mi))\n",
    "\n",
    "mi = pds.Series(mi)\n",
    "mi.index = train_df1.columns\n",
    "mi.sort_values(ascending=False).plot.bar(figsize=(10, 5))\n",
    "plt.ylabel('income_above_limit')\n",
    "plt.title(\"Mutual information between predictors and target\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e9754",
   "metadata": {
    "id": "a85e9754",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Gini index Calculation\n",
    "gi = []\n",
    "for feature in train_df1.columns:\n",
    "  feature_income = train_df_coded[0][[feature,\"income_above_limit\"]]\n",
    "\n",
    "  gini = 0\n",
    "      \n",
    "  for gender in feature_income[feature].unique():\n",
    "    filter = feature_income[feature_income[feature] == gender]\n",
    "    filter_size = len(filter)\n",
    "      \n",
    "    value_count = filter[\"income_above_limit\"].value_counts()\n",
    "\n",
    "    proportion = value_count / filter_size\n",
    "\n",
    "    gini_filter = 1 - sum (proportion ** 2)\n",
    "\n",
    "    gini += (len(filter)/ len(feature_income) * gini_filter)\n",
    "  gi.append(gini)\n",
    "\n",
    "  print(\"Gini Index for {} feature and target as income_above_limit is \".format(feature),gini)\n",
    "\n",
    "\n",
    "\n",
    "feature_column = train_df1.columns.to_list()\n",
    "gi.sort(reverse=True)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(feature_column, gi)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('income_above_limit')\n",
    "plt.title(\"Gini Index between predictors and target\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38031f24",
   "metadata": {
    "id": "38031f24"
   },
   "source": [
    "## 5.2 Report observations\n",
    "\n",
    "Write your observations from the results of each method. Clearly justify your choice of the method.\n",
    "\n",
    "Score 1 mark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9680e22-0d72-4951-a4b1-6eccadff12e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Observations:</b>\n",
    "\n",
    "\n",
    "**1. Information Gain:**\n",
    "\n",
    "Information Gain is applied on the train dataframe having 28 features.\n",
    "The target attribute 'income_above_limit' is categoricaly type. \n",
    "We separated out the discrete and continous features and Calculated IG of each attribute against target 'income above limit'.\n",
    "the IG plot is given above. We have set threshold of 0.02 and all features above this score is selected for below ML techniques.\n",
    "\n",
    "**2. Gini Index:**\n",
    "\n",
    "Gini is applied on the train dataframe having 28 features.\n",
    "We evaluated this method on all the features against target variable in below steps.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ea925",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Justification:</b>\n",
    "We have selected the Information Gain as the univariate filter in the ML techniques used below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c7a3a8",
   "metadata": {
    "id": "85c7a3a8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Short listed features after Data wrangling techniques\n",
    "\n",
    "feature_cols = ['occupation_code',\n",
    "                'education',\n",
    "                'working_week_per_year',\n",
    "                'industry_code', \n",
    "                'industry_code_main', \n",
    "                'age',\n",
    "                'tax_status', \n",
    "                'household_stat',\n",
    "                'total_employed', \n",
    "                'household_summary',\n",
    "                'stocks_status',\n",
    "                'gains',\n",
    "                'mig_year']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1173c",
   "metadata": {
    "id": "06f1173c"
   },
   "source": [
    "# 6. Implement Machine Learning Techniques\n",
    "\n",
    "Use any 2 ML algorithms\n",
    "1. Classification -- Decision Tree classifier\n",
    "\n",
    "2. Clustering -- kmeans\n",
    "\n",
    "3. Association Analysis\n",
    "\n",
    "4. Anomaly detection\n",
    "\n",
    "5. Textual data -- Naive Bayes classifier (not taught in this course)\n",
    "\n",
    "A clear justification have to be given for why a certain algorithm was chosen to address your problem.\n",
    "\n",
    "Score: 4 Marks (2 marks each for each algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040afed8",
   "metadata": {
    "id": "040afed8"
   },
   "source": [
    "## 6.1 ML technique 1 + Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042235d",
   "metadata": {
    "id": "7042235d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copied the dataframe to build decision tree, because second ML algorithm should get affected\n",
    "df = Dataframe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0292df31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics # Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import tree\n",
    "\n",
    "X = df[feature_cols] # Features\n",
    "y = df.income_above_limit # Target variable\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7224196a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "# clf = DecisionTreeClassifier(splitter='best', criterion='gini', max_depth=15, min_samples_split=2, min_samples_leaf=1000, max_features=None)\n",
    "clf = DecisionTreeClassifier(splitter='best', criterion='entropy', max_depth=15, min_samples_split=2, min_samples_leaf=1000, max_features=None)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X,y)\n",
    "\n",
    "# Predict the response for 30% test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Predicting the target for Actual testdata\n",
    "y_pred_actual_test_data = clf.predict(Test_Dataframe[feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334295c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detailed metrics can be found in Section 7:\n",
    "\n",
    "# Returns the mean accuracy on the given test data and labels.\n",
    "print(\"Model Accuracy: \", clf.score(X, y)) # Whole dataset score\n",
    "print(\"Model Accuracy: \", clf.score(X_test, y_test)) # Test data score\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "# print(\"Model Accuracy: \", metrics.accuracy_score(y_test, y_pred, normalize = True))\n",
    "print(\"Model Accuracy: \", metrics.accuracy_score(y_test, y_pred)) # 30% Test data score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581879ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO # Alternative for sklearn.externals.six\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data, max_depth=15,\n",
    "                feature_names=feature_cols, class_names=['Below','Above'],\n",
    "                label='all', filled=True, rounded=True,\n",
    "                special_characters=True, proportion=True)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "graph.write_png('income_dataset_decision_tree.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3e637",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>ML Technique 1: Justification for Decision Tree Classification:</b>\n",
    "\n",
    "Target for our supervised dataset is Categorical attribute, to which Classification method is one of the best approach. Hence, we used Decision tree Classification method as our first option!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f610ca8c",
   "metadata": {
    "id": "f610ca8c"
   },
   "source": [
    "## 6.2 ML technique 2 + Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75e079",
   "metadata": {
    "id": "9b75e079",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apriori: find frequent set of attributes pattern which implies income_above/below limit.\n",
    "#pip install mlxtend\n",
    "#Importing Libraries\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f_list = list()\n",
    "# pick Feature with < 15 Unique value to keep space complexity in limit.\n",
    "for entry in feature_cols:\n",
    "    if Dataframe[entry].nunique() < 30:\n",
    "        print(entry)\n",
    "        f_list.append(entry)\n",
    "f_list.append('income_above_limit')\n",
    "\n",
    "df1 = Dataframe[f_list].copy()\n",
    "display(df1)\n",
    "\n",
    "# one hot encoding\n",
    "df2 = pds.get_dummies(data=df1, columns=f_list)\n",
    "display(df2)\n",
    "\n",
    "\n",
    "freq_items = apriori(df2, min_support=0.7, use_colnames=True, verbose=1)\n",
    "#pds.set_option('display.max_colwidth', -1) \n",
    "display(freq_items)\n",
    "\n",
    "# creating asssociation rules\n",
    "rules = association_rules(freq_items, metric=\"confidence\", min_threshold=0.7)\n",
    "#pds.set_option('display.max_colwidth', -1) \n",
    "display(rules)\n",
    "\n",
    "\n",
    "#visualization\n",
    "plt.scatter(rules['support'], rules['confidence'], alpha=0.5)\n",
    "plt.xlabel('support')\n",
    "plt.ylabel('confidence')\n",
    "plt.title('Support vs Confidence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ea6f9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>ML Technique 2: Justification/Observation for Association Analysis:</b>\n",
    "\n",
    "1. Algo executed with min_support = 0.7 and min_confidence=0.7.\n",
    "2. Not a Single rule shows a pattern with Target = 'income_above_limit' which is business usecase requirement.\n",
    "3. All rules are finding pattern with Target = 'Income_below_limit'.\n",
    "4. Dataset which we are analysing is having very sparse entry (less than 5% ) with Target label = 'Income_above_limit' category, so Frequent pattern algo is not yielding meaningful set of rules for Target label = 'income_above_limit'.\n",
    "5. Based on Above observation we are dropping FP Algo for further analysis.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57940c",
   "metadata": {
    "id": "eb57940c"
   },
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "Compare the performance of the ML techniques used.\n",
    "\n",
    "Derive values for preformance study metrics like accuracy, precision, recall, F1 Score, AUC-ROC etc to compare the ML algos and plot them. A proper comparision based on different metrics should be done and not just accuracy alone, only then the comparision becomes authentic. You may use Confusion matrix, classification report, Word cloud etc as per the requirement of your application/problem.\n",
    "\n",
    "Score 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf06eb1",
   "metadata": {
    "id": "9bf06eb1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Calculation for accuracy, precision, recall, F1 Score, AUC-ROC, confusion matrix, classification report and plotting them\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report)\n",
    "\n",
    "#Calculating the Accuracy Decision tree classifier\n",
    "Accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of the model is: \",Accuracy)\n",
    "\n",
    "#Calculating the precision for Decision tree classifier\n",
    "Precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision of the model is: \",Precision)\n",
    "\n",
    "#Calculating the recall score for Decision tree classifier\n",
    "Recall_score = recall_score(y_test, y_pred)\n",
    "print(\"Recall score of the model is: \",Recall_score)    \n",
    "\n",
    "#Calculating the F1 score for Decision tree classifier\n",
    "F1_score = f1_score(y_test, y_pred)\n",
    "print(\"F1 score of the model is: \",F1_score)\n",
    "\n",
    "#Calculating the ROC_AUC for Decision tree classifier\n",
    "y_score = clf.predict_proba(X_test)[:, 1]\n",
    "ROC_AUC = roc_auc_score(y_test, y_score)\n",
    "print(\"ROC_AUC of the model is: \",ROC_AUC)\n",
    "\n",
    "#plot the metrics\n",
    "plt.bar(['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC'], [Accuracy, Precision, Recall_score, F1_score, ROC_AUC])\n",
    "plt.xlabel('Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Metrics for Decision Tree Classifier')\n",
    "plt.show()\n",
    "\n",
    "#Computing the confusion matrix for Decision tree classifier\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix of the model is \\n\",conf_matrix)\n",
    "\n",
    "#Computing the classification matrix for Decision tree classifier\n",
    "Class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification matrix of the model is \\n\",Class_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed0137",
   "metadata": {
    "id": "79ed0137"
   },
   "source": [
    "## 8. Solution\n",
    "\n",
    "What is the solution that is proposed to solve the business problem discussed in Section 1. Also share your learnings while working through solving the problem in terms of challenges, observations, decisions made etc.\n",
    "\n",
    "Score 2 Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3075283c-ff88-42c8-a87c-a9df550db945",
   "metadata": {
    "id": "68ab61d5"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Solution:</b>\n",
    "\n",
    "We can use this Trained model to classify our Bank customers and their financial capacity and accordingly design/prescribe credit card programs in future.\n",
    "The attributes that are helping to classify the Customers are dervied through the process of data preparation and feature/ML techniques we evaluated and applied above\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df6762-accd-4c71-bf0a-17e1aee1b2ca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Learnings:</b>\n",
    "\n",
    "Python versions, API's and usage issues.\n",
    "MAC and Windows compatibility issues.\n",
    "Few ML topics like Anomaly Detection are still in progress, but we have to cover those topics in advance.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RcDDQlfbZQ7E",
   "metadata": {
    "id": "RcDDQlfbZQ7E"
   },
   "source": [
    "##NOTE\n",
    "All Late Submissions will incur a penalty of -2 marks. Do ensure on time submission to avoid penalty.\n",
    "\n",
    "Good Luck!!!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
